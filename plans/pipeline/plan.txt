# Automated Narrative → Video System  
## Master Plan v1.2 (Source of Truth)

---

# 0. Answer to Repo Structure Question

Yes: this plan must specify repo structure (or at least a recommended structure and boundaries), because repo boundaries enforce product boundaries. Without this, “trend/media/video/orchestrator/canon” will drift into each other and become hard to scale or productize.

---

# 1. Purpose

This document defines the architecture, contracts, compute policy, repo structure, and productization boundaries for a system that:

1) Generates long-form narrative content (novels / episodic stories)  
2) Converts narrative into film-ready structure  
3) Produces preview videos locally and deterministically  
4) Optionally upgrades the same episode into high-quality cinematic video using external AI providers  
5) Maintains continuity across episodes and seasons  
6) Optimizes cost by defaulting to local compute whenever feasible  
7) Keeps components separable so multiple parts can become standalone products/services

This document is the single source of truth.  
All implementation decisions must follow it.

---

# 2. Design Principles

## Determinism First
The system must always produce a working preview video locally, with no dependency on external providers.

## Upgrade Without Rewrite
Upgrading quality must not require regenerating story, canon, or timing.

## Canon Over Generation
Generated content never defines truth. Canon Store defines truth.

## Structured Data Over Text Parsing
No module should interpret freeform prose from another module. Modules exchange JSON schemas only.

## Resume Anywhere
Every stage is restartable from stored artifacts with idempotent jobs.

## Local-First Cost Control
Every feature must have a local execution path. External AI is enhancement only.

## Product Boundaries by Contract
If a component can be fed by “any client” via a stable schema, it is productizable.

---

# 3. Recommended Repository Structure (and Why)

The goal is to keep “platform components” cleanly separable.

## 3.1 Current Repos (Existing)
- trend/  (Narrative Intelligence Layer)
- media/  (Asset Acquisition Layer)
- video/  (Rendering Layer)

## 3.2 New Repo (Required)
- orchestrator/  (Pipeline Brain + schemas + artifact registry client + job routing)

## 3.3 Optional Repo Split (If/When Productizing)
- world-engine/     (standalone writing/continuity product; may be trend evolved)
- video-engine/     (standalone render + media-resolve product; may merge media+video)
- canon-store/      (standalone canon DB + validation service; optional)
- validator/        (canon gate + policy enforcement; optional, can live in canon-store)
- asset-resolver/   (standalone asset resolution service; optional, can live in video-engine)
- schemas/          (pure schema package shared across repos if desired)

Rule of thumb:
- If it has a public API boundary: consider its own repo.
- If it’s purely internal glue: keep it in orchestrator.

---

# 4. High Level Pipeline (Artifact Boundaries)

IDEA  
→ Story Arc (Artifact)  
→ Episode Plan (Artifact)  
→ Script (Artifact)  
→ ShotList (Artifact)  
→ AssetManifest (Artifact)  
→ RenderPlan (Artifact)  
→ Preview Render Output (Artifact; Local Required)  
→ (Optional) HQ Render Output (Artifact; External Optional)  
→ Publish

Each arrow must be a durable storage checkpoint.

---

# 5. Core Data Contracts (Versioned Schemas)

All schemas are versioned. No breaking changes without version bump.

## 5.1 Project
Global settings and cost policy.
- id, title, genre, visual_style, target_duration
- render_profiles
- continuity_mode
- cost_policy

## 5.2 Canon (Structured Truth)
- characters, locations, world_rules, relationships
- timeline_events, persistent_states
Canon changes only via CanonDiff.

## 5.3 CanonDiff
- added_facts, modified_facts, removed_facts
- justification and provenance
- validation results

## 5.4 Episode
- episode_number, arc_reference
- summary, new_facts, unresolved_threads

## 5.5 Script
Scene-based screenplay structure.
- scenes: location/time, dialogue w speaker_id, actions, emotional tags
No camera instructions.

## 5.6 ShotList
Film-ready breakdown.
Per shot:
- duration, camera framing/movement, characters, expressions/poses
- environment notes, action beats
- audio intent (VO refs, SFX tags, music mood)

## 5.7 AssetManifest
Derived requirements from ShotList.
- required character packs, backgrounds, props
- VO line items (speaker_id, text, emotion/pacing tags)
- SFX/music needs
No provider references.

## 5.8 RenderPlan
Maps AssetManifest to actual sources + profile.
- profile: preview_local | standard_local | hq_providerX
- resolution/aspect/fps
- resolved asset URIs + fallbacks
- timing lock hash reference (from ShotList)

## 5.9 RenderOutput
- final video path/URI
- captions path/URI
- audio stems path/URI (optional)
- hashes, provenance links, lineage references

---

# 6. Canon & Continuity System (Platform-Critical)

Two layers:

## 6.1 Structured Canon Store (Truth Layer)
Hard facts and rules. All “truth queries” come from here.

## 6.2 Vector Memory (Recall Layer)
Semantic recall for richness (minor details, tone, vibe). Never authoritative.

## 6.3 Canon Gate (Continuity Validator)
Before finalizing Script and before approving CanonDiff:
1) Compare proposed facts vs Canon
2) Reject contradictions (or flag for human review)
3) Auto-repair if feasible (rewrite script segments, not canon)
4) Persist CanonDiff only if approved

## 6.4 Episode Context Assembly
Every generation step gets a strict context pack:
- Canon snapshot (relevant subset)
- Relevant past events (retrieved + structured)
- Last-episode summary + unresolved threads
No module reads raw old scripts directly.

---

# 7. Narrative Generation Flow (trend / world-engine)

Arc Generator  
→ Episode Planner  
→ Script Generator  
→ Continuity Editor (Canon Gate pre-check)  
→ CanonDiff Proposer  
→ Canon Gate Approver  
→ Script Finalizer

Outputs must pass Canon Gate.

---

# 8. Film Adaptation Flow (Script → ShotList)

Script Adapter  
→ Scene Breakdown  
→ Shot Planner  
→ Emotional Tagger  
→ Timing Estimator  
→ ShotList Finalizer (locks timing)

ShotList timing is LOCKED after approval and becomes the timeline authority for all renders.

---

# 9. Media Acquisition Flow (media / asset-resolver / video-engine)

Asset Resolver reads AssetManifest and returns resolved assets.

Priority order:
1) Local asset library
2) Cached generated assets
3) External search sources (configurable + rights-aware)
4) Generative fallback (local if feasible; otherwise external with budget)

All results must include licensing/provenance metadata.

---

# 10. Rendering Flow (video / video-engine)

## 10.1 Preview Render (Local Required)
- deterministic
- ffmpeg based
- captions required
- stable timing and repeatable templates
- renders even with placeholders

## 10.2 Standard Render (Local Preferred)
- higher resolution
- better local assets
- improved motion templates

## 10.3 HQ Render (External Optional)
- per-shot replacement (or per-scene)
- preserves timing and audio alignment
- does NOT change canon, script, or shot durations

---

# 11. Render Profiles (Upgrade System)

- preview_local: always works locally
- standard_local: better local assets, still local
- hq_providerX: external generation to replace preview clips

Render profile ONLY changes RenderPlan, never Script/ShotList/Canon.

---

# 12. Local vs External Routing Policy (Cost/Quality Control)

The orchestrator decides routing per task using cost_policy, current budgets, and availability.

## 12.1 Always Local (Required for System Correctness)
- Canon Store operations (read/write)
- Canon Gate validation
- Episode planning + script generation (using feasible local LLM)
- Shot planning + timing lock
- Preview render stitching + captions + loudness normalization
- Artifact registry + lineage tracking

## 12.2 Prefer Local (Fallback External Allowed)
- image generation for storyboards/background plates
- TTS voice generation (baseline quality)
- music beds and SFX selection/generation
Fallback external allowed only when explicitly enabled and budgeted.

## 12.3 External Optional Only (HQ Enhancements)
- cinematic text-to-video / diffusion video
- premium TTS voice acting
- photoreal animation, advanced motion
External tasks must not modify canon or timing.

## 12.4 Budget Enforcement
- Each run has a max budget.
- External stages require explicit approval policy (auto/threshold/manual).
- If external fails, pipeline falls back to preview_local output.

---

# 13. Scheduling & Queues

Queues separated by resource type:
- cpu_queue: stitching, validation, ffmpeg
- local_ai_queue: local LLM reasoning/writing
- media_queue: search, retrieval, caching, licensing checks
- external_ai_queue: HQ providers (rate limited + budgeted)

Jobs must be idempotent and restartable.

---

# 14. Artifact Registry (Reproducibility Backbone)

Every stage produces artifacts stored with:
- hash
- schema version
- parent references
- creation parameters
- compute origin (local/external)
- licensing/provenance (where applicable)

This enables: rerun, diff, audit, and “upgrade later”.

---

# 15. Quality Gates (Automated QA)

Before publish:
- Canon/Continuity Check
- Missing Asset Check (must have fallbacks)
- Runtime/Pacing Check (matches target window)
- Caption Sync Check
- Audio Loudness / Ducking Check
- Rights/License Check for external assets (if used)

Any failure blocks publish unless explicitly overridden.

---

# 16. Observability & Cost Tracking

Track per run:
- stage timings
- retries/failures
- local vs external usage
- token usage (if applicable)
- provider costs
- cache hit rates

Minimum: a run summary JSON + logs per stage.

---

# 17. Failure Rules

- HQ render failure → fallback to preview_local output
- Asset missing → placeholder + flag + continue
- Canon conflict → block finalization (no publish)
- Provider unavailable → degrade gracefully (no pipeline halt)

---

# 18. Productization Map (What Can Be Standalone Products)

This project intentionally supports multiple product surfaces.

## 18.1 Video Engine (Highly Productizable)
A “Render Package” in → video out platform.
- Inputs: ShotList + AssetManifest + RenderPlan
- Outputs: RenderOutput
No story intelligence inside.

## 18.2 Writing / World Engine (Productizable if canon-first)
A persistent world reasoning platform.
- Inputs: Canon + goals + constraints
- Outputs: Script + CanonDiff
No media/render awareness inside.

## 18.3 Canon Store (Productizable Infrastructure)
Truth layer for long-running AI systems.
- Canon CRUD + CanonDiff + snapshots

## 18.4 Continuity Validator / Canon Gate (Productizable)
Policy/truth enforcement that blocks contradictions and proposes repairs.

## 18.5 Asset Resolver (Productizable)
Find vs generate vs reuse decision engine with caching + licensing.

## 18.6 Orchestrator / Pipeline Runner (Productizable)
Resumable, artifact-based AI workflow engine with deterministic outputs.

Rule: If the component can be fed by “any client” via stable schema, it can be offered as a product.

---

# 19. Development Phases (Milestones)

## 19.0 Phase 0 — Minimal MVP (Local-Only Preview Pipeline, Schema-Compatible)

### Goal
Generate a 30–90 second episode end-to-end locally and produce a RenderOutput using preview_local.
All outputs must validate against the schemas and contracts defined in Sections 5, 10–12, 14–16, and 21 (RenderPackage v1).
No external AI providers are required or permitted in this phase.

### In Scope

**Artifacts (See Section 4 pipeline boundaries, Section 14 Artifact Registry)**
- Produce the full artifact chain: Script → ShotList → AssetManifest → RenderPlan (profile: preview_local) → RenderOutput
- Store every artifact in Artifact Registry format using local filesystem URIs (hash, schema_version, parent refs, creation params)
- Run summary JSON written at completion (stage timings, cache hits, errors)

**Canon (See Section 6)**
- Canon stored as a local JSON snapshot per project
- CanonDiff exists but is applied manually / via simple write (no auto-repair in MVP)
- Canon Gate is minimal: detect only hard contradictions (name, age, death status, location) and block finalization if violated

**Writing (See Section 7)**
- Script generation via either: (A) local LLM, or (B) deterministic template generator
- Output must conform to Script schema (Section 5.5) regardless of generation method

**Film Adaptation (See Section 8)**
- Script → ShotList adapter uses a small set of shot templates
- Every shot must include a duration; adapter must produce a `timing_lock_hash`

**Media Resolution (See Section 9)**
- Asset resolution from local assets directory only; no external search; no generative fallback required
- Placeholder assets used for any missing items — run must complete, not fail
- Minimum metadata per asset: `license_type` defaults to "proprietary_cleared" for local assets; placeholders flagged explicitly

**Audio**
- Local TTS acceptable; no-voice mode permitted for MVP
- Captions must be generated regardless of TTS availability
- Optional: single local background music track with basic loudness ducking

**Video (See Section 10.1)**
- Deterministic ffmpeg render: slideshow / animatic format
- Ken Burns-style motion on images is optional
- Captions burned-in or exported as sidecar .srt — both acceptable

**Orchestrator (See Section 13)**
- Single CLI command drives sequential stage execution
- Writes deterministic artifact folders; can resume from any completed checkpoint
- No distributed queues required; simple sequential runner is sufficient

### Out of Scope (Explicit)

- External HQ providers — Section 22 (no external calls at all)
- Multi-tenancy enforcement — Section 23 (single-tenant default only)
- Webhooks / callbacks — Section 21 (stub placeholder acceptable)
- Full rights gate blocking publish — Section 25.4 (local-only assumption applies; gate kept as stub)
- Distributed queues — Section 13 (sequential runner replaces queue workers for MVP)
- Advanced content safety scanning — Section 26 (basic text checks only; hooks must exist for later phases)

### Acceptance Criteria (Testable)

- [ ] One command produces: Script.json, ShotList.json, AssetManifest.json, RenderPlan.json, RenderOutput.json, final .mp4, captions file, and run_summary.json
- [ ] Re-running the same command with the same inputs yields identical output content hashes (deterministic within local constraints)
- [ ] Intentionally introducing a Canon contradiction causes Canon Gate to block finalization (no video produced)
- [ ] Missing assets are replaced with placeholders and the run completes without failure
- [ ] All generated artifacts are valid against their schemas and usable unchanged by Phase 1 and Phase 2 components

### Parallel Workstreams (For Implementation Agents)

**Workstream A — Orchestrator CLI + Artifact Folders**
- Inputs: Project config JSON
- Outputs: artifact directory layout, run_summary.json, resume checkpoints
- Done when: CLI runs end-to-end sequentially, writes all artifact folders, resumes correctly from each checkpoint

**Workstream B — Script → ShotList Adapter + Timing Lock**
- Inputs: Script.json (Section 5.5)
- Outputs: ShotList.json (Section 5.6) with durations + timing_lock_hash
- Done when: adapter produces valid ShotList from any valid Script; timing_lock_hash is stable across reruns; schema validates

**Workstream C — Preview Renderer + Captions**
- Inputs: AssetManifest.json (Section 5.7), RenderPlan.json (Section 5.8, profile: preview_local)
- Outputs: final .mp4, captions (.srt or burned-in), RenderOutput.json (Section 5.9)
- Done when: ffmpeg render is deterministic; missing asset slots are filled with placeholders; captions are present; output hashes are stable

**Workstream D — Minimal Canon Store + Minimal Gate**
- Inputs: Canon JSON snapshot, proposed CanonDiff (Section 5.3)
- Outputs: updated Canon snapshot or rejection error with contradiction details
- Done when: gate correctly blocks hard contradictions (name/age/death/location); valid diffs are accepted; Canon snapshot round-trips cleanly

---

Phase 1 – Deterministic preview pipeline end-to-end
- Script → ShotList → AssetManifest → preview_local render

Phase 2 – Canon enforcement and continuity gates  
- Canon Store + CanonDiff + Canon Gate integrated

Phase 3 – Asset resolver + caching + licensing  
- AssetManifest → resolved assets with provenance

Phase 4 – HQ provider integration + upgrade workflow  
- Replace preview clips with HQ clips while preserving timing

Phase 5 – Scale batching + cost optimization  
- queue tuning, cache optimization, run dashboards

Each phase must produce working videos.

---

# 20. Definition of Done

System can:
- Generate multi-episode story
- Maintain continuity via Canon Store + Gate
- Produce preview videos locally and deterministically
- Upgrade episodes to HQ video without rewriting story or timing
- Re-render identical results from stored artifacts
- Operate within configurable cost and routing policies
- Keep major components separable for productization

---

# 21. Public API — RenderPackage v1 Contract
(See also: Section 18.1 Video Engine, Section 5.8 RenderPlan, Section 11 Render Profiles)

The Video Engine / Render Service exposes a single external request interface.
Callers never send freeform prose — only validated JSON matching these schemas.

## 21.1 RenderPackage Request Object (v1)
- `request_id` — unique identifier for this render invocation (UUID)
- `idempotency_key` — client-supplied deduplication key; safe to retry on network failure
- `tenant_id` — required for multi-tenant deployments (see Section 23)
- `render_profile` — one of: preview_local | standard_local | hq_providerX
- `inputs` — structured object containing:
  - `shotlist_ref` — URI or inline ShotList artifact
  - `asset_manifest_ref` — URI or inline AssetManifest artifact
  - `render_plan_ref` — URI or inline RenderPlan artifact
- `desired_outputs` — list of requested outputs: video | captions | audio_stems | run_summary
- `callbacks` — optional webhook URL(s) for async status updates
- `logging_options` — trace level, redaction flags (see Section 24)

## 21.2 RenderPackage Response Object (v1)
- `request_id` — echoed from request
- `status` — one of: queued | in_progress | completed | failed | degraded
- `artifact_uris` — map of output name → URI (populated when completed)
- `lineage_hashes` — map of artifact name → content hash
- `run_summary` — structured object: stage timings, cost, cache hits, errors
- `errors` — list of structured error objects (code, message, stage, recoverable flag)

## 21.3 Contract Rules
- Services never parse freeform prose; only validated JSON matching these schemas.
- All inputs are refs (URIs) or inline schemas — never raw text strings.
- Idempotent: replaying the same `idempotency_key` returns the cached result without re-running.
- Render profile changes require a new request; they never mutate existing artifacts.

---

# 22. Provider Plugin Contract
(See also: Section 11 Render Profiles, Section 10.3 HQ Render, Section 5.8 RenderPlan)

All HQ providers (hq_providerX in RenderPlan) must implement this contract.
The orchestrator routes through this interface; no provider is called directly.

## 22.1 Capabilities Discovery
Each provider exposes a capabilities manifest:
- `provider_id` — unique slug (e.g., "runway_gen3", "kling_v1")
- `max_clip_duration_sec` — hard cap per generation call
- `supported_aspect_ratios` — list (e.g., ["16:9", "9:16", "1:1"])
- `supported_fps` — list of integers (e.g., [24, 30])
- `audio_support` — boolean
- `image_to_video` — boolean
- `text_to_video` — boolean
- `max_resolution` — string (e.g., "1920x1080")

## 22.2 Required Inputs Interface
- `shot_ref` — reference to a single shot from ShotList (duration, framing, prompt hints)
- `asset_refs` — optional image refs for reference frames or character consistency
- `prompt_constraints` — structured prompt fields (no raw paragraphs): subject, motion, style, mood
- `timing_lock_hash` — must match ShotList timing lock; provider may not alter shot duration

## 22.3 Cost Estimation Interface
Before execution, providers must expose a cost estimate call:
- Input: shot_ref + render_profile
- Output: estimated_cost_usd, estimated_duration_sec, confidence (low/medium/high)
- Orchestrator uses this to enforce budget policy (see Section 12.4) and trigger approval gates (see Section 27)

## 22.4 Retry & Backoff Semantics
- Providers must classify errors as: `transient` | `rate_limit` | `content_rejected` | `fatal`
- Transient + rate_limit: retry with exponential backoff (max 3 attempts, cap 60 sec)
- Content rejected: surface to content safety gate (see Section 26); do not retry automatically
- Fatal: log error, fallback to preview_local output, do not halt the pipeline

## 22.5 Output Naming & Provenance
- Outputs use deterministic naming: `{request_id}/{provider_id}/{shot_id}.mp4`
- Each output artifact records: provider_id, model_version, generation timestamp, cost, input hashes
- These feed directly into Artifact Registry (see Section 14)

---

# 23. Multi-Tenancy & Isolation Model
(Applicable when productizing Video Engine, Canon Store, or Orchestrator — see Section 18)

## 23.1 Minimum Viable Approach (MVP)
- Per-tenant artifact storage namespace: `artifacts/{tenant_id}/...`
- Per-tenant secrets scoped at the infrastructure level (separate secret paths, never shared env vars)
- Soft quota limits per tenant: max concurrent jobs, max daily external AI spend, max storage
- All logs tagged with `tenant_id`; log queries scoped by tenant — no cross-tenant log leakage

## 23.2 Later Enhancements
- Per-tenant encryption keys (envelope encryption for stored artifacts)
- Per-tenant rate limiting enforced at API gateway layer
- Configurable data retention and deletion policies: purge triggers removal of all artifacts, canon, and logs for that tenant
- Tenant-level cost dashboards and usage reports

## 23.3 Data Isolation Rules
- No cross-tenant artifact references allowed
- Canon Store must enforce tenant boundaries on all read and write paths
- Shared infrastructure (queues, worker pools) is acceptable as long as artifact namespacing is strictly enforced

---

# 24. Security & Secrets Management Baseline

## 24.1 Where Secrets Live
- Development: `.env` file (never committed; must be in `.gitignore`)
- Staging / Production: secrets vault (e.g., HashiCorp Vault, AWS Secrets Manager, or equivalent)
- No secrets in code, committed config files, or log output — anywhere in the pipeline

## 24.2 Least Privilege
- Each service and worker has its own credentials scoped to only what it needs
- External provider API keys scoped per service; no shared master keys across services
- Storage access: read-only paths for consumers, read-write only for producers

## 24.3 Rotation Policy
- External provider keys: rotate on any suspected exposure; scheduled rotation every 90 days recommended
- Internal service credentials: rotate on team member offboarding and on any breach indicator
- Rotation must not require pipeline downtime (use versioned secrets with a grace period overlap)

## 24.4 Log Redaction
- All logging pipelines must strip: API keys, tokens, webhook URLs, and tenant PII before writing
- Structured logging preferred (JSON); redaction applied at log-writer level, not ad hoc at call sites
- Audit logs (cost events, canon changes, publish events) are retained separately from debug logs

## 24.5 Dependency Pinning & Supply-Chain Scanning (Lightweight)
- All dependencies pinned to exact versions in lock files (no floating `^` or `~` in production images)
- Automated scanning: run a basic vulnerability scanner (e.g., `pip audit`, `npm audit`, `trivy`) in CI on every PR
- Alert on high/critical CVEs; block merge on critical findings

---

# 25. Rights & Provenance Requirements
(See also: Section 9 Media Acquisition, Section 14 Artifact Registry, Section 15 Quality Gates)

## 25.1 Allowed Asset Source Categories
- Explicitly purchased or licensed commercial assets (with a retrievable purchase record)
- Open-license assets (CC0, CC-BY with attribution, or equivalent permissive license)
- Locally generated assets (produced by the pipeline itself; no third-party rights claim applies)
- Assets explicitly cleared via written agreement

## 25.2 Minimum Metadata Fields (Per Asset)
Every asset in AssetManifest and Artifact Registry must carry:
- `license_type` — e.g., "CC0", "commercial_licensed", "generated_local", "proprietary_cleared"
- `attribution` — required for CC-BY and similar; empty string for CC0 or generated assets
- `purchase_record` — URI or reference to invoice or license document (for commercial assets)
- `provider_or_model` — source tool, platform, or model (e.g., "ElevenLabs v2", "Stable Diffusion XL")
- `retrieval_date` — ISO 8601 date of acquisition

## 25.3 Redistribution Policy for Generated Assets
- AI-generated assets must be reviewed for applicable model output license terms before redistribution
- Default policy: generated assets are for internal pipeline use unless explicitly cleared for redistribution
- `redistributable: true/false` flag in asset metadata; defaults to false

## 25.4 Unknown License Handling
- Assets with missing or unknown license metadata → **block from publish** (hard block, not a warning)
- During asset resolution (Section 9): reject unknown-license assets before adding to AssetManifest
- Quality Gate (Section 15) enforces this as a hard block at publish time
- Manual override requires explicit `rights_override` flag and approver ID recorded in the artifact

---

# 26. Content Safety Policy & Enforcement Points

## 26.1 Blocked Content Categories (High-Level)
- Sexual or explicit content
- Graphic violence beyond the project's configured rating threshold (set in Project config)
- Hate speech, harassment, or content targeting protected groups
- Illegal content (by jurisdiction of deployment)
- Deceptive synthetic media that could constitute fraud or impersonation of real individuals

## 26.2 Enforcement Stages
Safety checks are applied at four mandatory points:

| Stage                   | What is checked                              | Mode        |
|-------------------------|----------------------------------------------|-------------|
| Script generation       | Script content vs safety policy              | Fail closed |
| Shot prompt generation  | Prompt fields in ShotList                    | Fail closed |
| Asset retrieval         | Retrieved asset description + metadata       | Fail closed |
| Final output QA         | Rendered output (metadata + optional frame scan) | Fail closed |

## 26.3 Fail Closed vs Warn Only
- **Fail closed** (default): safety violation blocks the pipeline stage; no output produced; error surfaced to operator
- **Warn only** (opt-in, per project config): violation is logged and flagged but pipeline continues; publish is still blocked until manual review
- Warn-only mode must be explicitly enabled in Project config (`safety_mode: warn_only`); default is `fail_closed`

## 26.4 Provider-Side Rejections
- If an HQ provider rejects a generation request for content reasons (see Section 22.4): classify as `content_rejected`, log, escalate to operator, do not retry
- Never automatically reword prompts to bypass provider safety filtering

---

# 27. Human-in-the-Loop Checkpoints (Configurable)

## 27.1 Available Checkpoints
The pipeline supports optional human review gates at the following points:
- **approve_canon_diff** — Review and approve proposed CanonDiff before it is committed to Canon Store
- **approve_character_packs** — Review character asset packs before they are locked into AssetManifest
- **approve_shotlist_timing_lock** — Review ShotList timing before it is locked (timing lock is irreversible without a full re-render)
- **approve_external_spend** — Review estimated external provider cost before any external AI calls are dispatched

## 27.2 Configuration Mechanism
Checkpoints are toggled via Project config flags:
```
human_checkpoints:
  approve_canon_diff: true           # default: true
  approve_character_packs: false     # default: false
  approve_shotlist_timing_lock: true # default: true
  approve_external_spend: true       # default: true (when budget > threshold)
```
- When a checkpoint is enabled, the pipeline pauses and emits a review request (webhook, UI notification, or CLI prompt depending on deployment mode)
- The pipeline resumes only when an explicit approval signal is received
- Timeout policy: if no approval is received within a configurable window, the run is paused (not failed); it resumes on approval

## 27.3 Audit Trail
All human approval events are recorded in the Artifact Registry with: approver_id, timestamp, decision (approved/rejected), and optional reviewer notes.

---

# 28. Testing & CI/CD Requirements

## 28.1 Required Test Types
- **Schema validation unit tests** — every schema (ShotList, AssetManifest, RenderPlan, CanonDiff, RenderPackage) has roundtrip parse/serialize tests and rejection tests for invalid inputs
- **Golden render test** — a small deterministic clip (≤10 shots, fixed seed assets) that must produce bit-identical output on every run; used to catch renderer regressions
- **Continuity regression tests (canon gate)** — a curated set of known contradiction scenarios that the Canon Gate must correctly reject; and a set of valid diffs it must accept
- **Provider mock / plugin contract tests** — each provider plugin is tested against a mock that enforces the full plugin contract (capabilities, cost estimate, retry behavior, output naming — see Section 22)
- **Load / batch tests** — queue throughput test: N jobs submitted concurrently; verify completion rate, error rate, and timing within acceptable bounds (see Section 29 for targets)

## 28.2 Minimal CI Plan
On every PR:
1. Lint (language-appropriate: ruff / eslint / golangci-lint / etc.)
2. Schema validation unit tests
3. Canon gate regression tests
4. Provider contract tests (mocked)
5. Dependency vulnerability scan (see Section 24.5)

On merge to main:
- Full test suite including golden render test
- Artifact hash of golden output published to test registry for traceability

---

# 29. Performance Targets & Batching Strategy

## 29.1 Preview Render Targets (preview_local profile)
- Single episode (20–40 shots, ~10 min video): target completion in ≤ 15 minutes on reference hardware
- Per-shot render time (ffmpeg stitch): ≤ 30 seconds per shot
- TTS generation (local): ≤ 5 seconds per VO line
- Canon gate validation: ≤ 10 seconds per episode

## 29.2 Concurrency Goals
- Worker pools sized per queue type (see Section 13):
  - `cpu_queue`: 2–4 workers (CPU-bound; scale with cores)
  - `local_ai_queue`: 1 worker per GPU/accelerator; queue depth ≤ 8
  - `external_ai_queue`: configurable concurrency per provider (respect provider rate limits)
- Target: ≤ 20% queue wait time as a fraction of total run time under normal load

## 29.3 Caching Strategy
- Cache keys: deterministic hash of inputs (artifact content hashes, never file paths or timestamps)
- Cached artifact types: rendered shots, TTS audio, resolved assets, Canon snapshots
- Eviction policy: LRU with configurable max size; time-based expiry for externally sourced assets (respect license terms)
- Cache hit rate target: ≥ 60% for re-renders of the same episode with minor script changes

## 29.4 Job Sizing Guidelines
- Default granularity: **per-shot** jobs for rendering (enables partial re-render and fine-grained retry)
- Batch to **per-scene** only when provider pricing strongly favors batching (e.g., minimum billable unit exceeds 1 shot)
- Never batch across episodes — episode boundary equals artifact boundary

---

# 30. Versioning & Migration Policy for Schemas and Artifacts

## 30.1 Semantic Versioning for Schemas
- All schemas (ShotList, AssetManifest, RenderPlan, Canon, RenderPackage, etc.) follow semver: `MAJOR.MINOR.PATCH`
- PATCH: non-breaking fixes (documentation, constraint clarification)
- MINOR: backward-compatible additions (new optional fields, new enum values)
- MAJOR: breaking changes (removed fields, renamed required fields, changed types)
- Schema version is embedded in every artifact: `schema_version: "1.2.0"`

## 30.2 Forward / Backward Compatibility Expectations
- Readers must tolerate unknown fields (ignore gracefully) — this allows MINOR upgrades without requiring all readers to update simultaneously
- Readers must NOT silently ignore missing required fields — surface a clear, actionable error
- MAJOR version changes require a documented migration path before old artifacts are accepted by any service

## 30.3 Re-Rendering Old Artifacts
- Old artifacts are never mutated in place; re-render always creates new artifacts with the current schema version
- Both old and new artifacts are retained in the Artifact Registry with a lineage link between them
- Artifact Registry must preserve the original `schema_version` tag on every stored artifact indefinitely

## 30.4 Migration Scripts vs Render Adapters
- **Migration script**: transforms stored artifact data from schema vN to vN+1 without re-running pipeline stages; used for Canon, config, and metadata
- **Render adapter**: re-runs a pipeline stage using original inputs to produce new-schema outputs; used for media artifacts where data cannot be losslessly transformed
- Both must be idempotent and versioned alongside their respective schemas

## 30.5 Mixed-Version Handling Rules
- A pipeline run uses the schema version declared in its RenderPackage request
- Input artifacts at an older MINOR version: accepted transparently (backward compatible)
- Input artifacts at an older MAJOR version: pipeline rejects with a clear error message and migration instructions
- Mixed schema versions within a single run (e.g., ShotList v1.x + AssetManifest v2.x) are allowed only within the same MAJOR version family

---

# 31. Reference Directory Layout (Per Repo)

This is recommended guidance, not a hard requirement.
Repos should converge toward this structure for consistency and discoverability.

## 31.1 Shared Layout (All Repos)
```
/docs        — architecture notes, ADRs, runbooks, integration guides
/schemas     — versioned JSON/YAML schema definitions and validators
/tests       — all test types: unit, integration, golden, contract, load
/examples    — sample inputs and outputs for each major schema and flow
```

## 31.2 Orchestrator Repo
```
/pipelines   — pipeline stage definitions and workflow DAGs
/workers     — queue worker implementations (one subdirectory per queue type)
/providers   — provider plugin implementations (one subdirectory per provider)
/registry    — artifact registry client and lineage tracking
```

## 31.3 Narrative / World-Engine Repo (trend)
```
/generators  — arc, episode, and script generation modules
/canon       — Canon Store client, CanonDiff handlers, Canon Gate
/context     — episode context assembly logic
```

## 31.4 Media / Asset-Resolver Repo
```
/resolvers   — per-source resolvers (local library, cache, external search, generative)
/rights      — license metadata validators and rights gate
/cache       — asset cache management
```

## 31.5 Video / Render Repo
```
/renderers   — per-profile renderer implementations (preview_local, standard_local)
/templates   — reusable ffmpeg or render templates
/qa          — quality gate checks (caption sync, loudness, runtime pacing)
```

---

# 32. Control Plane (Management UI)
(See also: Section 13 Scheduling & Queues, Section 14 Artifact Registry, Section 16 Observability & Cost Tracking)

The Control Plane is a centralized dashboard and API that provides visibility and operational control over the system.
It is purely an observer and command dispatcher — it never executes pipeline logic directly.
The system must operate fully if the Control Plane is offline.

## 32.1 Run Dashboard
- Per-run view: run_id, site_id, tenant_id, current stage, elapsed time, errors, profile used
- Stage-level breakdown: start time, duration, exit status, artifact URIs produced
- Filter and search by: status, episode, tenant, site, date range

## 32.2 Queue Dashboard
- Real-time depth per queue type (cpu_queue, local_ai_queue, media_queue, external_ai_queue — see Section 13)
- Per-worker status: idle / active / draining / offline
- Queue throughput trend (jobs/hour) and estimated drain time

## 32.3 Artifact Lineage Viewer
- Visual graph of artifact parent-child relationships (see Section 14)
- Per-artifact detail: hash, schema_version, compute origin, cost, lineage links
- Diff view between two runs of the same episode

## 32.4 Cost Dashboard
- Per-run cost breakdown: local compute vs external provider spend
- Per-provider spend: tokens used by model/provider, cost per call, cumulative spend
- Cost per episode and cost per season rollups
- Budget threshold indicators (see Section 12.4)

## 32.5 Warning Panel
Surfaced alerts and soft failures:
- Canon Gate blocks (see Section 6.3)
- Repeated retries on any stage (threshold configurable)
- Provider outages or sustained high failure rates (see Section 22.4)
- Budget threshold warnings before hard limit is reached
- Disk pressure or host health degradation (see Section 37)

## 32.6 Drill-Down Views
- **Run view**: full stage timeline, logs per stage, artifacts produced
- **Stage view**: inputs, outputs, errors, retry history
- **Host view**: current load, queue worker status, recent job history (see Section 37)

## 32.7 Control Plane Rules
- Does not execute pipeline stages or write artifacts directly
- Sends signed command requests to Ops Agents on each host (see Section 34)
- Receives telemetry from workers and the artifact registry passively
- Must degrade gracefully: if Control Plane is unreachable, local pipeline execution continues unaffected

---

# 33. Alerts & Notifications
(See also: Section 16 Observability & Cost Tracking, Section 32 Control Plane, Section 37 Host Health Model)

## 33.1 Alert Categories
| Alert Type             | Trigger Condition                                               |
|------------------------|-----------------------------------------------------------------|
| `job_failed`           | A pipeline stage exits with a non-recoverable error            |
| `excessive_retries`    | Retry count for a stage exceeds configured threshold           |
| `provider_unavailable` | External provider returns fatal errors for N consecutive calls |
| `budget_exceeded`      | Run or daily spend crosses configured hard limit               |
| `canon_blocked`        | Canon Gate rejects a finalization attempt                      |
| `host_down`            | Host heartbeat not received within expected interval           |
| `disk_full`            | Disk usage on any host exceeds configured high-water mark      |

## 33.2 Notification Requirements
- Every alert message must include: `run_id`, `stage`, `site_id`, `alert_type`, `severity`, `timestamp`
- Deep link to the relevant Control Plane view (run, host, or queue — see Section 32)
- At least one phone-friendly delivery channel (push notification, messaging app, or email — implementation flexible)
- Multiple channels may be configured; routing rules configurable per alert category and severity

## 33.3 Reliability Rules
- **Deduplication window**: identical alert (same type + run_id + stage) suppressed for a configurable window (default: 5 minutes)
- **Severity thresholds**: configurable per category; below threshold → logged only, not dispatched
- Alert delivery must not depend on the pipeline being healthy (use a separate lightweight emitter)

---

# 34. Safe Remote Operations
(See also: Section 17 Failure Rules, Section 23 Multi-Tenancy & Isolation, Section 32 Control Plane)

Operators trigger actions from the Control Plane. All actions are executed by a lightweight **Ops Agent** running on each host — no direct remote shell access from the UI.

## 34.1 Allowed Operator Actions
- **restart worker** — gracefully stop and restart a specific queue worker process
- **drain worker** — stop accepting new jobs; finish current job; then go idle
- **pause queue** — halt job dispatch for a named queue; jobs remain queued
- **resume queue** — re-enable job dispatch for a paused queue
- **cancel run** — mark a run as cancelled; stop all in-flight stages for that run_id
- **retry stage** — re-enqueue a specific failed stage for a given run_id

## 34.2 Ops Agent
- Runs as a lightweight process on every host in the fleet
- Receives signed command requests from the Control Plane over a local or authenticated channel
- Executes only the predefined actions above — no arbitrary command execution
- Reports action result (success / failure / timeout) back to the Control Plane

## 34.3 Safety Rules
- No direct SSH or shell access from the Control Plane UI to any host
- All command requests must be cryptographically signed; Ops Agent rejects unsigned requests
- RBAC enforced: roles define which actions are permitted per user (e.g., read-only, operator, admin)
- Destructive actions (cancel run, restart worker) require explicit confirmation before dispatch
- Every action is audited: user identity, timestamp, action, target host/run, and outcome logged immutably

---

# 35. Multi-Site Deployment Strategy
(See also: Section 13 Scheduling & Queues, Section 36 Site-Aware Scheduling, Section 32 Control Plane)

The system is designed to operate across multiple geographic sites (target: 3–4 sites, 20+ servers total).

## 35.1 Site Model
- Each site is an **independent compute cell**: local workers, local artifact storage, local queues
- A `site_id` is attached to every run, artifact, host record, and log entry
- Sites operate autonomously — pipeline execution continues even if inter-site communication is interrupted
- Control Plane provides global visibility across sites but is not required for local execution (see Section 32.7)

## 35.2 Data Locality Rules
- Prefer local execution: jobs run on the same site where their primary input artifacts are stored
- Avoid cross-site transfer of large media artifacts during active pipeline runs
- **Metadata and logs only** are replicated across sites (asynchronously)
- Artifact content replication is explicit and opt-in (e.g., for backup or promotion to another site)

## 35.3 External Provider Calls
- HQ provider calls (see Section 22) may originate from any site that has network access and the required credentials
- Outputs are stored at the originating site and linked in the global artifact registry

## 35.4 Failure Isolation
- A site going offline does not halt other sites
- Runs pinned to an offline site are paused; they resume when the site recovers or are manually migrated
- Cross-references to Section 17 failure rules apply at the site level as well

---

# 36. Site-Aware Scheduling & Storage Locality
(Extends Section 13 Scheduling & Queues. See also: Section 35 Multi-Site Deployment, Section 14 Artifact Registry)

## 36.1 Scheduling Rules
- The orchestrator schedules each job to the site where the job's primary input artifacts are stored (locality-first)
- Cross-site scheduling is allowed only when explicitly requested by an operator or when the origin site is unavailable
- Worker pools are site-scoped: a cpu_queue worker on Site A only executes jobs enqueued for Site A

## 36.2 Artifact Pinning
- Large artifacts (video clips, audio, image packs) are pinned to their origin site
- Artifact registry records `site_id` for every artifact; cross-site references are flagged
- Promoting an artifact to another site is an explicit, audited operation

## 36.3 Metadata Replication
- Schema records, run summaries, cost logs, and lineage links are replicated globally (async, eventually consistent)
- Job routing decisions use locally cached metadata; full sync is not required before dispatch

## 36.4 Cache Locality
- Asset caches (see Section 29.3) are local to each site
- Cache misses on one site do not trigger cross-site fetches; the site resolves locally or falls back per Section 9 priority order
- Cache population from a promoted artifact is an explicit step, not automatic

---

# 37. Host Health Model
(See also: Section 16 Observability & Cost Tracking, Section 32 Control Plane, Section 33 Alerts & Notifications, Section 36 Site-Aware Scheduling)

## 37.1 Per-Host Metrics
Collected by the Ops Agent (see Section 34.2) and reported to the Control Plane:
- CPU load (1-min and 5-min averages)
- Memory usage (used / total)
- Disk usage per volume (used / total; alert at configurable high-water mark)
- Queue worker heartbeat (last heartbeat timestamp per worker)
- Temperature (optional; surfaced as informational only)

## 37.2 Per-Service Metrics
- Heartbeat interval: each service emits a periodic liveness signal
- Last successful job: timestamp of most recently completed job
- Failure rate: rolling count of failed jobs over a configurable window

## 37.3 Health States
| State      | Meaning                                                              |
|------------|----------------------------------------------------------------------|
| `healthy`  | All metrics within normal thresholds; heartbeat current              |
| `degraded` | One or more metrics outside normal range but service is still running|
| `offline`  | Heartbeat missed beyond timeout threshold; assumed unreachable       |

## 37.4 Usage
- **Scheduling**: orchestrator avoids dispatching jobs to `degraded` or `offline` hosts (see Section 36)
- **Alerts**: state transitions to `degraded` or `offline` trigger `host_down` alerts (see Section 33)
- **Control Plane**: host health state displayed in real time in host drill-down view (see Section 32.6)

---

END OF DOCUMENT
